
```
Benchmark           Mode  Cnt  Score   Error  Units
DemoBenchmark.busr  avgt   10  2.758 ± 0.093   s/op
DemoBenchmark.erka  avgt   10  2.423 ± 0.039   s/op
DemoBenchmark.conf  avgt   10  1.779 ± 0.018   s/op
DemoBenchmark.actr  avgt   10  1.430 ± 0.073   s/op
DemoBenchmark.demo  avgt   10  1.154 ± 0.056   s/op
DemoBenchmark.slim  avgt   10  1.112 ± 0.024   s/op
DemoBenchmark.thin  avgt   10  0.968 ± 0.027   s/op
DemoBenchmark.lite  avgt   10  0.813 ± 0.024   s/op
DemoBenchmark.func  avgt   10  0.742 ± 0.012   s/op
```

.Number of Beans vs. Startup Time
image::https://docs.google.com/spreadsheets/d/1SqvJ-cjIYVvHUmG61CzwylxwwnjeXark855JEGESbbs/pubchart?oid=2090464856&amp;format=image[]

Legend:

* Slth: same as "busr" but adds Sleuth
* Zuul: same as "busr" but with Zuul proxy
* Busr: same as "conf" but adds Spring Cloud Bus and Rabbit
* Erka: same as "actr" sample but with Eureka client
* Conf: same as "actr" sample plus config client
* Actr: same as "demo" sample plus Actuator
* Demo: vanilla Spring Boot MVC app with one endpoint (no Actuator)
* Slim: same thing but explicitly `@Imports` all configuration
* Thin: reduce the `@Imports` down to a set of 4 that are needed for the endpoint
* Lite: copy the imports from "thin" and make them into hard-coded, unconditional configuration
* Func: extract the configuration methods from "lite" and register bits of it using the function bean API

N.B. The "thin" sample has `@EnableWebMvc` (implicitly), but "lite"
and "func" pulled the relevant features of that out into a separate
class (so a few beans were dropped).

|===
| sample | configs | beans | startup(millis)

| slth | 176| 460 | 5366
| zuul | 181| 495 | 4336
| busr | 151| 389 | 2758
| erka | 127| 310 | 2423
| conf | 100| 245 | 1779
| actr | 72 | 183 | 1430
| demo | 32 | 108 | 1154
| slim | 31 | 103 | 1112
| thin | 14 | 60  | 968
| lite | 4  | 30  | 813
| func | 1  | 25  | 742

|===

== Outliers

These samples didn't fit the trend. The must be doing something dumb because they start up way slower than the others.

```
Benchmark           Mode  Cnt  Score   Error  Units
DemoBenchmark.slth  avgt   10  5.366 ± 0.088   s/op
DemoBenchmark.zuul  avgt   10  4.336 ± 0.098   s/op
```

Here's an explanation for the "slth" result. The difference between the "slth" sample and the "demo" is 460 beans compared to 108, and 4 aspects. The aspects have 5 `@Around` advices between them plus a reflection based hand coded pointcut, that's probably worth 2 `@Arounds` or so. That's enough to account for the difference between the Sleuth sample and the trend line if there is a fixed cost of roughly 600ns per bean per advice.  The cost is reflection used to compute pointcuts on 460 beans: `460*600*7/1000 = 1932 ms`, nearly 2 seconds. If you take the "demo" sample and add an `@Aspect` with 1, 2, 3, 4, 5, 6 etc advice methods, you can measure that the cost of the aspects in that sample is indeed roughly 600ns per bean (the pointcut doesn't have to match anything - it still has to be evaluated for all beans).

== Laptop (carbon)

```
Benchmark           Mode  Cnt  Score   Error  Units
DemoBenchmark.demo  avgt   10  1.697 ± 0.081   s/op
DemoBenchmark.slim  avgt   10  1.673 ± 0.098   s/op
DemoBenchmark.thin  avgt   10  1.446 ± 0.061   s/op
DemoBenchmark.lite  avgt   10  1.203 ± 0.072   s/op
DemoBenchmark.func  avgt   10  1.150 ± 0.056   s/op
```