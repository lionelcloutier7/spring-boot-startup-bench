
```
Benchmark           Mode  Cnt  Score   Error  Units Beans
DemoBenchmark.erka  avgt   10  3.137 ± 0.049   s/op 494
DemoBenchmark.busr  avgt   10  2.596 ± 0.071   s/op 409
DemoBenchmark.conf  avgt   10  1.636 ± 0.029   s/op 250
DemoBenchmark.acjd  avgt   10  1.540 ± 0.049   s/op 226
DemoBenchmark.actr  avgt   10  1.316 ± 0.060   s/op 186
DemoBenchmark.jdbc  avgt   10  1.237 ± 0.050   s/op 147
DemoBenchmark.demo  avgt   10  1.056 ± 0.040   s/op 111
DemoBenchmark.slim  avgt   10  1.003 ± 0.011   s/op 105
DemoBenchmark.thin  avgt   10  0.855 ± 0.028   s/op 60
DemoBenchmark.lite  avgt   10  0.694 ± 0.015   s/op 30
DemoBenchmark.func  avgt   10  0.652 ± 0.017   s/op 25
```

.Number of Beans vs. Startup Time
image::https://docs.google.com/spreadsheets/d/1SqvJ-cjIYVvHUmG61CzwylxwwnjeXark855JEGESbbs/pubchart?oid=2090464856&amp;format=image[]

Legend:

* Erka: same as "busr" sample but with Eureka client
* Busr: same as "conf" but adds Spring Cloud Bus and Rabbit
* Conf: same as "actr" sample plus config client
* Acjd: same as "actr" sample plus JDBC
* Actr: same as "demo" sample plus Actuator
* Jdbc: same as "demo" sample plus JDBC
* Demo: vanilla Spring Boot MVC app with one endpoint (no Actuator)
* Slim: same thing but explicitly `@Imports` all configuration
* Thin: reduce the `@Imports` down to a set of 4 that are needed for the endpoint
* Lite: copy the imports from "thin" and make them into hard-coded, unconditional configuration
* Func: extract the configuration methods from "lite" and register bits of it using the function bean API

N.B. The "thin" sample has `@EnableWebMvc` (implicitly), but "lite"
and "func" pulled the relevant features of that out into a separate
class (so a few beans were dropped).

== Old Data

(Boot 1.5.4 without `-noverify`)

|===
| sample | configs | beans | startup(millis)

| slth | 176| 460 | 5366
| zuul | 181| 495 | 4336
| busr | 151| 389 | 2758
| erka | 127| 310 | 2423
| conf | 100| 245 | 1779
| actr | 72 | 183 | 1430
| demo | 32 | 108 | 1154
| slim | 31 | 103 | 1112
| thin | 14 | 60  | 968
| lite | 4  | 30  | 813
| func | 1  | 25  | 742

|===

== Outliers

These samples didn't fit the trend. The must be doing something dumb because they start up way slower than the others.

```
Benchmark           Mode  Cnt  Score   Error  Units Beans
DemoBenchmark.slth  avgt   10  5.099 ± 0.043   s/op 455
DemoBenchmark.zuul  avgt   10  4.137 ± 0.173   s/op 481
```

Here's an explanation for the "slth" result. The difference between the "slth" sample and the "demo" is 460 beans compared to 108, and 4 aspects. The aspects have 5 `@Around` advices between them plus a reflection based hand coded pointcut, that's probably worth 2 `@Arounds` or so. That's enough to account for the difference between the Sleuth sample and the trend line if there is a fixed cost of roughly 600ns per bean per advice.  The cost is reflection used to compute pointcuts on 460 beans: `460*600*7/1000 = 1932 ms`, nearly 2 seconds. If you take the "demo" sample and add an `@Aspect` with 1, 2, 3, 4, 5, 6 etc advice methods, you can measure that the cost of the aspects in that sample is indeed roughly 600ns per bean (the pointcut doesn't have to match anything - it still has to be evaluated for all beans).

== Laptop (carbon)

```
Benchmark           Mode  Cnt  Score   Error  Units
DemoBenchmark.demo  avgt   10  1.697 ± 0.081   s/op
DemoBenchmark.slim  avgt   10  1.673 ± 0.098   s/op
DemoBenchmark.thin  avgt   10  1.446 ± 0.061   s/op
DemoBenchmark.lite  avgt   10  1.203 ± 0.072   s/op
DemoBenchmark.func  avgt   10  1.150 ± 0.056   s/op
```